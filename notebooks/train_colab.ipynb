{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Sentinel: Credit Card Fraud Detection Training\n",
                "\n",
                "This notebook trains an XGBoost model for fraud detection with MLflow experiment tracking via DagsHub.\n",
                "\n",
                "**Requirements:**\n",
                "- Run on Google Colab with GPU runtime\n",
                "- DagsHub account for experiment tracking"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Setup Environment"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Install dependencies\n",
                "!pip install -q xgboost mlflow dagshub scikit-learn pandas matplotlib seaborn"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "import json\n",
                "import warnings\n",
                "warnings.filterwarnings('ignore')\n",
                "\n",
                "import numpy as np\n",
                "import pandas as pd\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "\n",
                "from sklearn.metrics import (\n",
                "    classification_report,\n",
                "    confusion_matrix,\n",
                "    precision_recall_curve,\n",
                "    average_precision_score,\n",
                "    roc_auc_score,\n",
                "    f1_score,\n",
                "    precision_score,\n",
                "    recall_score\n",
                ")\n",
                "\n",
                "import xgboost as xgb\n",
                "import mlflow\n",
                "import dagshub\n",
                "\n",
                "print(f\"XGBoost version: {xgb.__version__}\")\n",
                "print(f\"MLflow version: {mlflow.__version__}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Mount Google Drive & Load Data"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from google.colab import drive\n",
                "drive.mount('/content/drive')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Set your data path - UPDATE THIS to your actual path\n",
                "# Option 1: Upload processed data to Google Drive\n",
                "# Option 2: Download directly from Kaggle\n",
                "\n",
                "DATA_PATH = '/content/drive/MyDrive/Sentinel/data/processed'\n",
                "\n",
                "# If data is not in Drive, download from Kaggle and preprocess\n",
                "if not os.path.exists(DATA_PATH):\n",
                "    print(\"Data not found in Drive. Downloading from Kaggle...\")\n",
                "    \n",
                "    # Install Kaggle\n",
                "    !pip install -q kaggle\n",
                "    \n",
                "    # Upload your kaggle.json or set credentials\n",
                "    # from google.colab import files\n",
                "    # files.upload()  # Upload kaggle.json\n",
                "    # !mkdir -p ~/.kaggle && mv kaggle.json ~/.kaggle/ && chmod 600 ~/.kaggle/kaggle.json\n",
                "    \n",
                "    # Download dataset\n",
                "    !kaggle datasets download -d mlg-ulb/creditcardfraud -p /content/data/raw --unzip\n",
                "    \n",
                "    # Preprocess\n",
                "    from sklearn.model_selection import train_test_split\n",
                "    from sklearn.preprocessing import StandardScaler\n",
                "    \n",
                "    df = pd.read_csv('/content/data/raw/creditcard.csv')\n",
                "    \n",
                "    # Scale Amount and Time\n",
                "    scaler = StandardScaler()\n",
                "    df[['Amount', 'Time']] = scaler.fit_transform(df[['Amount', 'Time']])\n",
                "    \n",
                "    # Stratified split\n",
                "    train_df, temp_df = train_test_split(df, test_size=0.3, stratify=df['Class'], random_state=42)\n",
                "    val_df, test_df = train_test_split(temp_df, test_size=0.5, stratify=temp_df['Class'], random_state=42)\n",
                "    \n",
                "    # Save\n",
                "    os.makedirs('/content/data/processed', exist_ok=True)\n",
                "    train_df.to_csv('/content/data/processed/train.csv', index=False)\n",
                "    val_df.to_csv('/content/data/processed/val.csv', index=False)\n",
                "    test_df.to_csv('/content/data/processed/test.csv', index=False)\n",
                "    \n",
                "    DATA_PATH = '/content/data/processed'\n",
                "    print(f\"Data preprocessed and saved to {DATA_PATH}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load processed data\n",
                "train_df = pd.read_csv(f'{DATA_PATH}/train.csv')\n",
                "val_df = pd.read_csv(f'{DATA_PATH}/val.csv')\n",
                "test_df = pd.read_csv(f'{DATA_PATH}/test.csv')\n",
                "\n",
                "print(f\"Train: {len(train_df):,} rows\")\n",
                "print(f\"Val: {len(val_df):,} rows\")\n",
                "print(f\"Test: {len(test_df):,} rows\")\n",
                "\n",
                "# Check class distribution\n",
                "print(f\"\\nTrain fraud ratio: {train_df['Class'].mean()*100:.4f}%\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Prepare features and labels\n",
                "TARGET = 'Class'\n",
                "FEATURES = [col for col in train_df.columns if col != TARGET]\n",
                "\n",
                "X_train = train_df[FEATURES]\n",
                "y_train = train_df[TARGET]\n",
                "\n",
                "X_val = val_df[FEATURES]\n",
                "y_val = val_df[TARGET]\n",
                "\n",
                "X_test = test_df[FEATURES]\n",
                "y_test = test_df[TARGET]\n",
                "\n",
                "# Calculate scale_pos_weight for class imbalance\n",
                "scale_pos_weight = (y_train == 0).sum() / (y_train == 1).sum()\n",
                "print(f\"Scale pos weight: {scale_pos_weight:.1f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Initialize DagsHub MLflow Tracking"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Initialize DagsHub connection\n",
                "# This will prompt for authentication on first run\n",
                "dagshub.init(repo_owner='hammadmunir959', repo_name='my-first-repo', mlflow=True)\n",
                "\n",
                "# Set experiment name\n",
                "mlflow.set_experiment('sentinel-fraud-detection')\n",
                "\n",
                "print(f\"MLflow tracking URI: {mlflow.get_tracking_uri()}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Define Hyperparameters"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# XGBoost Hyperparameters\n",
                "PARAMS = {\n",
                "    'n_estimators': 200,\n",
                "    'max_depth': 6,\n",
                "    'learning_rate': 0.1,\n",
                "    'subsample': 0.8,\n",
                "    'colsample_bytree': 0.8,\n",
                "    'min_child_weight': 1,\n",
                "    'gamma': 0,\n",
                "    'reg_alpha': 0,\n",
                "    'reg_lambda': 1,\n",
                "    'scale_pos_weight': scale_pos_weight,\n",
                "    'random_state': 42,\n",
                "    'n_jobs': -1,\n",
                "    'eval_metric': 'aucpr',\n",
                "    'early_stopping_rounds': 20,\n",
                "}\n",
                "\n",
                "print(\"Hyperparameters:\")\n",
                "for k, v in PARAMS.items():\n",
                "    print(f\"  {k}: {v}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Train Model with MLflow Logging"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def compute_metrics(y_true, y_pred, y_prob):\n",
                "    \"\"\"Compute all evaluation metrics.\"\"\"\n",
                "    return {\n",
                "        'precision': precision_score(y_true, y_pred),\n",
                "        'recall': recall_score(y_true, y_pred),\n",
                "        'f1_score': f1_score(y_true, y_pred),\n",
                "        'roc_auc': roc_auc_score(y_true, y_prob),\n",
                "        'average_precision': average_precision_score(y_true, y_prob),\n",
                "    }\n",
                "\n",
                "def plot_confusion_matrix(y_true, y_pred, save_path=None):\n",
                "    \"\"\"Plot and optionally save confusion matrix.\"\"\"\n",
                "    cm = confusion_matrix(y_true, y_pred)\n",
                "    plt.figure(figsize=(8, 6))\n",
                "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
                "                xticklabels=['Normal', 'Fraud'],\n",
                "                yticklabels=['Normal', 'Fraud'])\n",
                "    plt.title('Confusion Matrix')\n",
                "    plt.ylabel('Actual')\n",
                "    plt.xlabel('Predicted')\n",
                "    plt.tight_layout()\n",
                "    if save_path:\n",
                "        plt.savefig(save_path, dpi=150)\n",
                "    plt.show()\n",
                "\n",
                "def plot_precision_recall_curve(y_true, y_prob, save_path=None):\n",
                "    \"\"\"Plot precision-recall curve.\"\"\"\n",
                "    precision, recall, _ = precision_recall_curve(y_true, y_prob)\n",
                "    ap = average_precision_score(y_true, y_prob)\n",
                "    \n",
                "    plt.figure(figsize=(8, 6))\n",
                "    plt.plot(recall, precision, 'b-', linewidth=2, label=f'AP = {ap:.4f}')\n",
                "    plt.fill_between(recall, precision, alpha=0.3)\n",
                "    plt.xlabel('Recall')\n",
                "    plt.ylabel('Precision')\n",
                "    plt.title('Precision-Recall Curve')\n",
                "    plt.legend(loc='upper right')\n",
                "    plt.grid(True, alpha=0.3)\n",
                "    plt.tight_layout()\n",
                "    if save_path:\n",
                "        plt.savefig(save_path, dpi=150)\n",
                "    plt.show()\n",
                "\n",
                "def plot_feature_importance(model, feature_names, top_n=15, save_path=None):\n",
                "    \"\"\"Plot top N feature importances.\"\"\"\n",
                "    importance = model.feature_importances_\n",
                "    indices = np.argsort(importance)[-top_n:]\n",
                "    \n",
                "    plt.figure(figsize=(10, 8))\n",
                "    plt.barh(range(len(indices)), importance[indices], color='steelblue')\n",
                "    plt.yticks(range(len(indices)), [feature_names[i] for i in indices])\n",
                "    plt.xlabel('Feature Importance')\n",
                "    plt.title(f'Top {top_n} Feature Importances')\n",
                "    plt.tight_layout()\n",
                "    if save_path:\n",
                "        plt.savefig(save_path, dpi=150)\n",
                "    plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Start MLflow run\n",
                "with mlflow.start_run(run_name='xgboost_baseline'):\n",
                "    \n",
                "    # Log parameters\n",
                "    mlflow.log_params(PARAMS)\n",
                "    mlflow.log_param('train_size', len(X_train))\n",
                "    mlflow.log_param('val_size', len(X_val))\n",
                "    mlflow.log_param('test_size', len(X_test))\n",
                "    mlflow.log_param('n_features', len(FEATURES))\n",
                "    \n",
                "    # Initialize model\n",
                "    model = xgb.XGBClassifier(\n",
                "        n_estimators=PARAMS['n_estimators'],\n",
                "        max_depth=PARAMS['max_depth'],\n",
                "        learning_rate=PARAMS['learning_rate'],\n",
                "        subsample=PARAMS['subsample'],\n",
                "        colsample_bytree=PARAMS['colsample_bytree'],\n",
                "        min_child_weight=PARAMS['min_child_weight'],\n",
                "        gamma=PARAMS['gamma'],\n",
                "        reg_alpha=PARAMS['reg_alpha'],\n",
                "        reg_lambda=PARAMS['reg_lambda'],\n",
                "        scale_pos_weight=PARAMS['scale_pos_weight'],\n",
                "        random_state=PARAMS['random_state'],\n",
                "        n_jobs=PARAMS['n_jobs'],\n",
                "        eval_metric=PARAMS['eval_metric'],\n",
                "        early_stopping_rounds=PARAMS['early_stopping_rounds'],\n",
                "        use_label_encoder=False,\n",
                "    )\n",
                "    \n",
                "    # Train model\n",
                "    print(\"Training XGBoost model...\")\n",
                "    model.fit(\n",
                "        X_train, y_train,\n",
                "        eval_set=[(X_val, y_val)],\n",
                "        verbose=20\n",
                "    )\n",
                "    \n",
                "    # Predictions\n",
                "    y_val_pred = model.predict(X_val)\n",
                "    y_val_prob = model.predict_proba(X_val)[:, 1]\n",
                "    \n",
                "    y_test_pred = model.predict(X_test)\n",
                "    y_test_prob = model.predict_proba(X_test)[:, 1]\n",
                "    \n",
                "    # Compute metrics\n",
                "    val_metrics = compute_metrics(y_val, y_val_pred, y_val_prob)\n",
                "    test_metrics = compute_metrics(y_test, y_test_pred, y_test_prob)\n",
                "    \n",
                "    # Log validation metrics\n",
                "    for name, value in val_metrics.items():\n",
                "        mlflow.log_metric(f'val_{name}', value)\n",
                "    \n",
                "    # Log test metrics\n",
                "    for name, value in test_metrics.items():\n",
                "        mlflow.log_metric(f'test_{name}', value)\n",
                "    \n",
                "    # Print results\n",
                "    print(\"\\n\" + \"=\"*60)\n",
                "    print(\"VALIDATION METRICS\")\n",
                "    print(\"=\"*60)\n",
                "    for name, value in val_metrics.items():\n",
                "        print(f\"  {name}: {value:.4f}\")\n",
                "    \n",
                "    print(\"\\n\" + \"=\"*60)\n",
                "    print(\"TEST METRICS\")\n",
                "    print(\"=\"*60)\n",
                "    for name, value in test_metrics.items():\n",
                "        print(f\"  {name}: {value:.4f}\")\n",
                "    \n",
                "    # Plot and save artifacts\n",
                "    os.makedirs('/content/reports', exist_ok=True)\n",
                "    \n",
                "    # Confusion matrix\n",
                "    plot_confusion_matrix(y_test, y_test_pred, '/content/reports/confusion_matrix.png')\n",
                "    mlflow.log_artifact('/content/reports/confusion_matrix.png')\n",
                "    \n",
                "    # Precision-Recall curve\n",
                "    plot_precision_recall_curve(y_test, y_test_prob, '/content/reports/pr_curve.png')\n",
                "    mlflow.log_artifact('/content/reports/pr_curve.png')\n",
                "    \n",
                "    # Feature importance\n",
                "    plot_feature_importance(model, FEATURES, save_path='/content/reports/feature_importance.png')\n",
                "    mlflow.log_artifact('/content/reports/feature_importance.png')\n",
                "    \n",
                "    # Save model\n",
                "    os.makedirs('/content/models', exist_ok=True)\n",
                "    model.save_model('/content/models/model.json')\n",
                "    mlflow.log_artifact('/content/models/model.json')\n",
                "    \n",
                "    # Log model with signature\n",
                "    from mlflow.models.signature import infer_signature\n",
                "    signature = infer_signature(X_train, model.predict(X_train))\n",
                "    mlflow.xgboost.log_model(model, 'model', signature=signature)\n",
                "    \n",
                "    # Save metrics as JSON\n",
                "    all_metrics = {\n",
                "        'validation': val_metrics,\n",
                "        'test': test_metrics\n",
                "    }\n",
                "    with open('/content/reports/metrics.json', 'w') as f:\n",
                "        json.dump(all_metrics, f, indent=2)\n",
                "    mlflow.log_artifact('/content/reports/metrics.json')\n",
                "    \n",
                "    print(\"\\n\" + \"=\"*60)\n",
                "    print(\"Training complete! Artifacts logged to MLflow.\")\n",
                "    print(\"=\"*60)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Classification Report"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"\\nClassification Report (Test Set):\")\n",
                "print(classification_report(y_test, y_test_pred, target_names=['Normal', 'Fraud']))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7. Download Model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Option 1: Save to Google Drive\n",
                "!mkdir -p /content/drive/MyDrive/Sentinel/models\n",
                "!cp /content/models/model.json /content/drive/MyDrive/Sentinel/models/\n",
                "print(\"Model saved to Google Drive: /Sentinel/models/model.json\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Option 2: Download directly\n",
                "from google.colab import files\n",
                "files.download('/content/models/model.json')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 8. View Experiments on DagsHub\n",
                "\n",
                "Visit your DagsHub repository to see the logged experiments:\n",
                "https://dagshub.com/hammadmunir959/my-first-repo/experiments"
            ]
        }
    ],
    "metadata": {
        "accelerator": "GPU",
        "colab": {
            "gpuType": "T4",
            "provenance": []
        },
        "kernelspec": {
            "display_name": "Python 3",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.10.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 0
}